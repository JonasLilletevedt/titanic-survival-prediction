{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f240ec",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction: 02 - Modeling\n",
    "*Date: 14.09.2025*\n",
    "*Author: Jonas Lilletvedt*\n",
    "\n",
    "--- \n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Objective\n",
    "\n",
    "This notebook marks the second part of the Titanic Survival Prediction project: model training and evaluation. We will build directly on the pre-processing pipeline constructed in `01_data_cleaning_and_feature_engineering.ipynb`. The primary objective is to systematically train, evaluate, and tune a selection of models to find the optimal balance between predictive performance and interpretability. \n",
    "\n",
    "1. **Preparation and Pipeline Reconstruction:** We will begin by loading datasets and rebuilding the pre-processing pipeline from the previous notebook to ensure a consistent and reproducible environment.\n",
    "2. **Baseline Model Evaluation:** A selection of diverse classification models (e.g., K-NN, Random Forest, Gradient Boosting) will be used as baseline and later help us identify strategies for optimization. \n",
    "3. **Hyperparameter Tuning:** The best-performing model from the baseline evaluation will undergo systematic hyperparameter tuning using GridSearchCV. The goal is to maximize its predictive performance based on our primary evaluation metric, the F1-Score. The reasoning for selecting this metric is detailed in the section below.\n",
    "4. **Final Analysis and Iteration:** Finally, we will analyze the results and train the optimized model on the full dataset to generate final predictions for a Kaggle submission. We will conclude by exploring potential avenues for future improvements, such as alternative modeling techniques or further feature engineering.\n",
    "\n",
    "Our end goal is twofold: to develop a model that is competitive on the Kaggle leaderboard, while remaining understandable enough to provide insights into the factors that influence survival. The process will be iterative, allowing us to refine our approach based on model performance. \n",
    "\n",
    "Beyond the Leaderboard: A Focus on Interpretability\n",
    "Unlike many competition-driven projects that prioritize raw predictive power above all else, this analysis places a strong emphasis on interpretability. While achieving a high Kaggle score is a key objective, it is equally important to build a model whose decision-making process can be understood. By favoring models and features that provide clear insights—for instance, by showing why certain passengers were more likely to survive—we aim to produce not just a \"black box\" predictor, but a meaningful analysis of a historical event. This balanced approach ensures that our final result is both powerful and insightful.\n",
    "\n",
    "### 1.2 Defining Success: Beyond Simple Accuracy\n",
    "\n",
    "While accuracy is a straightforward metric, it can be misleading. A simple \"naive\" model that predicts survival for all females and death for all males achieves an accuracy of approximately 78.7% on the training data. Any model we build must therefore significantly outperform this baseline to be considered valuable.\n",
    "\n",
    "To truly understand our model's performance, we must select the right evaluation metrics for the task. The most useful evaluation metric depends on the specific problem you are trying to solve. For example, in medical screening for a virus, achieving a high Recall is more important than high Precision. We would want to identify as many infected people as possible, even if it means accepting some false positives. In other situations, however, overall Accuracy might be the primary goal.\n",
    "Applying this to the Titanic problem, our context is one of historical analysis and balanced prediction. There is no real-world cost that makes predicting a death incorrectly worse than predicting a survival incorrectly. Our goal is not just to be right, but to build a model that is equally effective at identifying both those who survived and those who did not.\n",
    "For this reason, we need metrics that reward this balance and are not skewed by the class distribution. While we will still consider overall Accuracy, our primary metric for model evaluation is:\n",
    "*   F1-Score: The harmonic mean of precision and recall. Provides an overall score between precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b805e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Data Loading and Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fa503",
   "metadata": {},
   "source": [
    "### 2.1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4ee99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn tools for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e8d39",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe708d8",
   "metadata": {},
   "source": [
    "### 2.2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86e82f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv('../data/01_raw/train.csv')\n",
    "df_test = pd.read_csv('../data/01_raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4cfea",
   "metadata": {},
   "source": [
    "### 2.3. Initial Inspection\n",
    "\n",
    "A quick inspection to check the datasets are loaded properly and as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaddf75",
   "metadata": {},
   "source": [
    "**Datasets Shape:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "680658e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (891, 12)\n",
      "Test data shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of each dataset\n",
    "print(f'Training data shape: {df_train.shape}')\n",
    "print(f'Test data shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a23e27",
   "metadata": {},
   "source": [
    "**Data Preview:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8c79928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check five first rows in df_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "602514ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check five first rows in df_test\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af898b38",
   "metadata": {},
   "source": [
    "### 2.4. Prepare Data for Modeling\n",
    "\n",
    "The datasets are loaded as expected. We will now separate our training data into two distinct objects:\n",
    "*   **X:** A DataFrame containing all predictor variables.\n",
    "*   **y:** A Series containing the target variable `Survived`, which we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e66db967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate predictors from target\n",
    "X_train = df_train.drop('Survived', axis=1)\n",
    "y_train = df_train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d9a20",
   "metadata": {},
   "source": [
    "To avoid making changes on the original test data we will copy it to a separate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1c601b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to avoid making changes to the original dataset\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc5179",
   "metadata": {},
   "source": [
    "Display the shapes of our new variables to ensure the separation and copy worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "936598c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (891, 11)\n",
      "Shape of y_train: (891,)\n",
      "Shape of X_test: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d946f7e",
   "metadata": {},
   "source": [
    "### 2.5. Reconstruct the Pre-processing Pipeline\n",
    "\n",
    "To make this notebook self-contained and reproducible, we will now redefine the custom transformers and the full `grand_pipeline` that were built and validated in the previous notebook. All the data transformation logic is encapsulated within a single code cell below. \n",
    "\n",
    "**A Note on Modularity vs. Narrative Flow**\n",
    "\n",
    "In a production-level software project, this logic would typically be defined in a separate python-file to be imported. However, for this project, which is designed to be a linear, narrative-driven analysis, I have chosen to explicitly include the code here. This approach ensures that the notebook tells the complete story, every change made to the code later does not affect the previous notebooks, and is well explained at that current moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af618220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleExtractor(BaseEstimator,  TransformerMixin):\n",
    "    def __init__(self, rare_threshold=10):\n",
    "        self.rare_threshold = rare_threshold\n",
    "\n",
    "        # Titles with same meaning\n",
    "        self.title_synonym_mapping_ = {\n",
    "            'Mlle.': 'Miss.',\n",
    "            'Ms.': 'Miss.',\n",
    "            'Mme.': 'Mrs.'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        titles = X['Name'].str.extract(pat=' ([A-Za-z]+\\.)', expand=False)\n",
    "        titles = titles.replace(self.title_synonym_mapping_)\n",
    "        self.non_rare_titles_ = titles.value_counts()[lambda x: x >= self.rare_threshold].index\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Copy to avoid modifying original\n",
    "        X_copy = X.copy()\n",
    "        # Extract title \n",
    "        X_copy['Title_feat'] = X_copy['Name'].str.extract(pat=' ([A-Za-z]+\\.)', expand=False)\n",
    "        \n",
    "        # Swap titles with 'Rare' and synonym\n",
    "        X_copy['Title_feat'] = X_copy['Title_feat'].replace(self.title_synonym_mapping_)\n",
    "\n",
    "\n",
    "        X_copy['Title_feat'] = X_copy['Title_feat'].apply(lambda x: x if x in self.non_rare_titles_ else 'Rare')\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class FamilySurvialRateExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_surname=True, smooth_factor=1):\n",
    "        self.drop_surname = drop_surname\n",
    "        self.smooth_factor = smooth_factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_temp = pd.concat([X, y], axis=1)\n",
    "\n",
    "        X_temp['FamilyID_temp'] = self.__get_family_id(X)\n",
    "\n",
    "\n",
    "        self.family_stats_ = X_temp.groupby('FamilyID_temp').agg(\n",
    "            FamilySize_temp = ('Survived', 'size'), \n",
    "            FamilySurvivalCount_temp = ('Survived', 'sum')\n",
    "        )\n",
    "    \n",
    "        self.global_survival_rate_ = y.mean()\n",
    "        self.training_index_ = X.index\n",
    "        self.y_train_ = y\n",
    "\n",
    "        self.alone_survival_rate_ = self.family_stats_[self.family_stats_['FamilySize_temp'] == 1]['FamilySurvivalCount_temp'].mean()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        X_copy['FamilyID_temp'] = self.__get_family_id(X)\n",
    "\n",
    "        X_copy = X_copy.merge(self.family_stats_, on='FamilyID_temp', how='left')\n",
    "\n",
    "        # Different calculation for passengers from training set -- avoid data leakage\n",
    "        if X.index.equals(self.training_index_):\n",
    "            X_copy['FamilySurvivalCount_temp'] -= self.y_train_\n",
    "            X_copy['FamilySize_temp'] -= 1\n",
    "\n",
    "        numerator = X_copy['FamilySurvivalCount_temp'] \n",
    "        denominator = X_copy['FamilySize_temp']\n",
    "        \n",
    "        # Apply smoothing\n",
    "        smoothed_numerator = numerator + (self.smooth_factor * self.global_survival_rate_)\n",
    "        smoothed_denominator = denominator + self.smooth_factor\n",
    "\n",
    "        # Calculate FamilySurvivalRate\n",
    "        X_copy['FamilySurvivalRate_feat'] = smoothed_numerator / smoothed_denominator\n",
    "\n",
    "        # For passengers without a family from training\n",
    "        X_copy['FamilySurvivalRate_feat'] = X_copy['FamilySurvivalRate_feat'].fillna(self.global_survival_rate_)\n",
    "\n",
    "        # For passengers which travel alone we will overwrite global_survival_rate_\n",
    "        is_alone_mask = (X_copy.groupby('FamilyID_temp')['FamilyID_temp'].transform('count') == 1) & (X_copy['FamilySize_temp'] == 0 | X_copy['FamilySize_temp'].isna())\n",
    "\n",
    "        X_copy.loc[is_alone_mask, 'FamilySurvivalRate_feat'] = self.alone_survival_rate_\n",
    "\n",
    "        # Columns to drop\n",
    "        columns_to_drop = ['FamilySurvivalCount_temp', 'FamilySize_temp', 'FamilyID_temp']\n",
    "        \n",
    "        # Clean df\n",
    "        X_copy = X_copy.drop(columns_to_drop, axis=1)\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "    def __get_surname(self, X):\n",
    "        # Extract surname\n",
    "        data = X.copy()\n",
    "        return data['Name'].str.extract(pat=r'^(.+)?,', expand=False)\n",
    "    \n",
    "    def __get_family_id(self, X):\n",
    "        return self.__get_surname(X) + '_' + X['Pclass'].astype(str)\n",
    "\n",
    "class AgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_temp = X.copy()\n",
    "        self.median_by_title_ = X_temp.groupby('Title_feat')['Age'].median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Age'] = X_copy['Age'].fillna(X_copy['Title_feat'].map(self.median_by_title_))\n",
    "        return X_copy\n",
    "\n",
    "class CabinLocationExtractor:\n",
    "    def __init__(self, drop_original=True):\n",
    "        self.drop_original = True\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Deck_feat'] = X_copy['Cabin'].str.extract(pat=r'^([A-Za-z])?', expand=False).fillna('U')\n",
    "        X_copy['Zone_feat'] = pd.to_numeric(\n",
    "            X_copy['Cabin'].str.extract(pat=r'([0-9]+)', expand=False),\n",
    "            errors='coerce'\n",
    "            )\n",
    "        return X_copy\n",
    "    \n",
    "feature_engineering_pipeline = Pipeline(steps=[\n",
    "    ('title_extractor', TitleExtractor()),\n",
    "    ('age_imputer', AgeImputer()),\n",
    "    ('cabin_location_extractor', CabinLocationExtractor())\n",
    "])\n",
    "\n",
    "FARE_TO_LOG_TRANS = ['Fare']\n",
    "CAT_FEATURES = ['Sex',\n",
    "                        'Embarked',\n",
    "                        'Pclass',\n",
    "                        'Title_feat',\n",
    "                        'Deck_feat'\n",
    "                        ]\n",
    "AGE_TO_BIN = ['Age']\n",
    "ZONE_TO_BIN = ['Zone_feat']\n",
    "\n",
    "\n",
    "fare_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(np.log1p)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Simple function for pd.cut\n",
    "def apply_pd_cut(X, bins, labels):\n",
    "    series = pd.Series(X[:, 0])\n",
    "    binned_series = pd.cut(series, bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "    return binned_series.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Bins for Age\n",
    "# Infant: 0-5, Child: 6-12, young-adult: 13-25, adult: 26-50, elder: 51->\n",
    "AGE_BINS = [0, 5, 12, 25, 50, np.inf]\n",
    "AGE_LABELS = ['Infant', 'Child', 'Young-Adult', 'Adult', 'Senior']\n",
    "\n",
    "age_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('binner', FunctionTransformer(apply_pd_cut, kw_args={'bins': AGE_BINS, 'labels': AGE_LABELS})),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "def apply_bin_zone(X, n_bins):\n",
    "    series = pd.Series(X[:, 0])\n",
    "    labels  = [f'Q{i}' for i in range(1, n_bins + 1)]\n",
    "    binned_series = pd.qcut(series, q=n_bins, labels=labels, duplicates='drop')\n",
    "\n",
    "    # Convert nans to unknown\n",
    "    binned_series = binned_series.cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "    return binned_series.to_numpy().reshape(-1, 1)\n",
    "\n",
    "zone_pipeline = Pipeline(steps=[\n",
    "    ('bin_with_missing_values', FunctionTransformer(apply_bin_zone, kw_args={'n_bins': 8})),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('fare_log', fare_pipeline, FARE_TO_LOG_TRANS),\n",
    "        ('cat', categorical_pipeline, CAT_FEATURES),\n",
    "        ('age_binned', age_pipeline, AGE_TO_BIN),\n",
    "        ('zone_binned', zone_pipeline, ZONE_TO_BIN)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "grand_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', feature_engineering_pipeline),\n",
    "    ('preprocessing', preprocessor)\n",
    "])  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic-survival-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
