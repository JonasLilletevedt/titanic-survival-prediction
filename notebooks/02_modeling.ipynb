{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f240ec",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction: 02 - Modeling\n",
    "*Date: 14.09.2025*\n",
    "*Author: Jonas Lilletvedt*\n",
    "\n",
    "--- \n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Objective\n",
    "\n",
    "This notebook marks the second part of the Titanic Survival Prediction project: model training and evaluation. We will build directly on the pre-processing pipeline constructed in `01_data_cleaning_and_feature_engineering.ipynb`. The primary objective is to systematically train, evaluate, and tune a selection of models to find the optimal balance between predictive performance and interpretability. \n",
    "\n",
    "1. **Preparation and Pipeline Reconstruction:** We will begin by loading datasets and rebuilding the pre-processing pipeline from the previous notebook to ensure a consistent and reproducible environment.\n",
    "2. **Baseline Model Evaluation:** A selection of diverse classification models (e.g., K-NN, Random Forest, Gradient Boosting) will be used as baseline and later help us identify strategies for optimization. \n",
    "3. **Hyperparameter Tuning:** The best-performing model from the baseline evaluation will undergo systematic hyperparameter tuning using GridSearchCV. The goal is to maximize its predictive performance based on our primary evaluation metric, the F1-Score. The reasoning for selecting this metric is detailed in the section below.\n",
    "4. **Final Analysis and Iteration:** Finally, we will analyze the results and train the optimized model on the full dataset to generate final predictions for a Kaggle submission. We will conclude by exploring potential avenues for future improvements, such as alternative modeling techniques or further feature engineering.\n",
    "\n",
    "Our end goal is twofold: to develop a model that is competitive on the Kaggle leaderboard, while remaining understandable enough to provide insights into the factors that influence survival. The process will be iterative, allowing us to refine our approach based on model performance. \n",
    "\n",
    "Beyond the Leaderboard: A Focus on Interpretability\n",
    "Unlike many competition-driven projects that prioritize raw predictive power above all else, this analysis places a strong emphasis on interpretability. While achieving a high Kaggle score is a key objective, it is equally important to build a model whose decision-making process can be understood. By favoring models and features that provide clear insights—for instance, by showing why certain passengers were more likely to survive—we aim to produce not just a \"black box\" predictor, but a meaningful analysis of a historical event. This balanced approach ensures that our final result is both powerful and insightful.\n",
    "\n",
    "### 1.2 Defining Success: Beyond Simple Accuracy\n",
    "\n",
    "While accuracy is a straightforward metric, it can be misleading. A simple \"naive\" model that predicts survival for all females and death for all males achieves an accuracy of approximately 78.7% on the training data. Any model we build must therefore significantly outperform this baseline to be considered valuable.\n",
    "\n",
    "To truly understand our model's performance, we must select the right evaluation metrics for the task. The most useful evaluation metric depends on the specific problem you are trying to solve. For example, in medical screening for a virus, achieving a high Recall is more important than high Precision. We would want to identify as many infected people as possible, even if it means accepting some false positives. In other situations, however, overall Accuracy might be the primary goal.\n",
    "Applying this to the Titanic problem, our context is one of historical analysis and balanced prediction. There is no real-world cost that makes predicting a death incorrectly worse than predicting a survival incorrectly. Our goal is not just to be right, but to build a model that is equally effective at identifying both those who survived and those who did not.\n",
    "For this reason, we need metrics that reward this balance and are not skewed by the class distribution. While we will still consider overall Accuracy, our primary metric for model evaluation is:\n",
    "*   F1-Score: The harmonic mean of precision and recall. Provides an overall score between precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b805e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Data Loading and Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fa503",
   "metadata": {},
   "source": [
    "### 2.1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a4ee99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e8d39",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe708d8",
   "metadata": {},
   "source": [
    "### 2.2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "86e82f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv('../data/01_raw/train.csv')\n",
    "df_test = pd.read_csv('../data/01_raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4cfea",
   "metadata": {},
   "source": [
    "### 2.3. Initial Inspection\n",
    "\n",
    "A quick inspection to check the datasets are loaded properly and as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaddf75",
   "metadata": {},
   "source": [
    "**Datasets Shape:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "680658e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (891, 12)\n",
      "Test data shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of each dataset\n",
    "print(f'Training data shape: {df_train.shape}')\n",
    "print(f'Test data shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a23e27",
   "metadata": {},
   "source": [
    "**Data Preview:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c79928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check five first rows in df_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "602514ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check five first rows in df_test\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af898b38",
   "metadata": {},
   "source": [
    "### 2.4. Prepare Data for Modeling\n",
    "\n",
    "The datasets are loaded as expected. We will now separate our training data into two distinct objects:\n",
    "*   **X:** A DataFrame containing all predictor variables.\n",
    "*   **y:** A Series containing the target variable `Survived`, which we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e66db967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate predictors from target\n",
    "X_train = df_train.drop('Survived', axis=1)\n",
    "y_train = df_train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d9a20",
   "metadata": {},
   "source": [
    "To avoid making changes on the original test data we will copy it to a separate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1c601b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to avoid making changes to the original dataset\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc5179",
   "metadata": {},
   "source": [
    "Display the shapes of our new variables to ensure the separation and copy worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "936598c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (891, 11)\n",
      "Shape of y_train: (891,)\n",
      "Shape of X_test: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d946f7e",
   "metadata": {},
   "source": [
    "### 2.5. Reconstruct the Pre-processing Pipeline\n",
    "\n",
    "To make this notebook self-contained and reproducible, we will now redefine the custom transformers and the full `grand_pipeline` that were built and validated in the previous notebook. All the data transformation logic is encapsulated within a single code cell below. \n",
    "\n",
    "**A Note on Modularity vs. Narrative Flow**\n",
    "\n",
    "In a production-level software project, this logic would typically be defined in a separate python-file to be imported. However, for this project, which is designed to be a linear, narrative-driven analysis, I have chosen to explicitly include the code here. This approach ensures that the notebook tells the complete, end-to-end story. Any future changes or improvements made in later stages of the project does not affect the logic or result of previous notebooks, preserving the integrity of each step of of our analysis. Furthermore, it allows any modifications or improvements to be documented and explained at the precise moment they are introduced, for a more natural progression.\n",
    "\n",
    "All data transformation logic is therefore encapsulated within the single code cell below, which can be collapsed for easier reading of the modeling work that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "af618220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn tools for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TitleExtractor(BaseEstimator,  TransformerMixin):\n",
    "    def __init__(self, rare_threshold=10):\n",
    "        self.rare_threshold = rare_threshold\n",
    "\n",
    "        # Titles with same meaning\n",
    "        self.title_synonym_mapping_ = {\n",
    "            'Mlle.': 'Miss.',\n",
    "            'Ms.': 'Miss.',\n",
    "            'Mme.': 'Mrs.'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        titles = X['Name'].str.extract(pat=' ([A-Za-z]+\\.)', expand=False)\n",
    "        titles = titles.replace(self.title_synonym_mapping_)\n",
    "        self.non_rare_titles_ = titles.value_counts()[lambda x: x >= self.rare_threshold].index\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Copy to avoid modifying original\n",
    "        X_copy = X.copy()\n",
    "        # Extract title \n",
    "        X_copy['Title_feat'] = X_copy['Name'].str.extract(pat=' ([A-Za-z]+\\.)', expand=False)\n",
    "        \n",
    "        # Swap titles with 'Rare' and synonym\n",
    "        X_copy['Title_feat'] = X_copy['Title_feat'].replace(self.title_synonym_mapping_)\n",
    "\n",
    "\n",
    "        X_copy['Title_feat'] = X_copy['Title_feat'].apply(lambda x: x if x in self.non_rare_titles_ else 'Rare')\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class FamilySurvialRateExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_surname=True, smooth_factor=1):\n",
    "        self.drop_surname = drop_surname\n",
    "        self.smooth_factor = smooth_factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_temp = pd.concat([X, y], axis=1)\n",
    "\n",
    "        X_temp['FamilyID_temp'] = self.__get_family_id(X)\n",
    "\n",
    "\n",
    "        self.family_stats_ = X_temp.groupby('FamilyID_temp').agg(\n",
    "            FamilySize_temp = ('Survived', 'size'), \n",
    "            FamilySurvivalCount_temp = ('Survived', 'sum')\n",
    "        )\n",
    "    \n",
    "        self.global_survival_rate_ = y.mean()\n",
    "        self.training_index_ = X.index\n",
    "        self.y_train_ = y\n",
    "\n",
    "        self.alone_survival_rate_ = self.family_stats_[self.family_stats_['FamilySize_temp'] == 1]['FamilySurvivalCount_temp'].mean()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        X_copy['FamilyID_temp'] = self.__get_family_id(X)\n",
    "\n",
    "        X_copy = X_copy.merge(self.family_stats_, on='FamilyID_temp', how='left')\n",
    "\n",
    "        # Different calculation for passengers from training set -- avoid data leakage\n",
    "        if X.index.equals(self.training_index_):\n",
    "            X_copy['FamilySurvivalCount_temp'] -= self.y_train_\n",
    "            X_copy['FamilySize_temp'] -= 1\n",
    "\n",
    "        numerator = X_copy['FamilySurvivalCount_temp'] \n",
    "        denominator = X_copy['FamilySize_temp']\n",
    "        \n",
    "        # Apply smoothing\n",
    "        smoothed_numerator = numerator + (self.smooth_factor * self.global_survival_rate_)\n",
    "        smoothed_denominator = denominator + self.smooth_factor\n",
    "\n",
    "        # Calculate FamilySurvivalRate\n",
    "        X_copy['FamilySurvivalRate_feat'] = smoothed_numerator / smoothed_denominator\n",
    "\n",
    "        # For passengers without a family from training\n",
    "        X_copy['FamilySurvivalRate_feat'] = X_copy['FamilySurvivalRate_feat'].fillna(self.global_survival_rate_)\n",
    "\n",
    "        # For passengers which travel alone we will overwrite global_survival_rate_\n",
    "        is_alone_mask = (X_copy.groupby('FamilyID_temp')['FamilyID_temp'].transform('count') == 1) & (X_copy['FamilySize_temp'] == 0 | X_copy['FamilySize_temp'].isna())\n",
    "\n",
    "        X_copy.loc[is_alone_mask, 'FamilySurvivalRate_feat'] = self.alone_survival_rate_\n",
    "\n",
    "        # Columns to drop\n",
    "        columns_to_drop = ['FamilySurvivalCount_temp', 'FamilySize_temp', 'FamilyID_temp']\n",
    "        \n",
    "        # Clean df\n",
    "        X_copy = X_copy.drop(columns_to_drop, axis=1)\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "    def __get_surname(self, X):\n",
    "        # Extract surname\n",
    "        data = X.copy()\n",
    "        return data['Name'].str.extract(pat=r'^(.+)?,', expand=False)\n",
    "    \n",
    "    def __get_family_id(self, X):\n",
    "        return self.__get_surname(X) + '_' + X['Pclass'].astype(str)\n",
    "\n",
    "class AgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_temp = X.copy()\n",
    "        self.median_by_title_ = X_temp.groupby('Title_feat')['Age'].median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Age'] = X_copy['Age'].fillna(X_copy['Title_feat'].map(self.median_by_title_))\n",
    "        return X_copy\n",
    "\n",
    "class CabinLocationExtractor:\n",
    "    def __init__(self, drop_original=True):\n",
    "        self.drop_original = True\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Deck_feat'] = X_copy['Cabin'].str.extract(pat=r'^([A-Za-z])?', expand=False).fillna('U')\n",
    "        X_copy['Zone_feat'] = pd.to_numeric(\n",
    "            X_copy['Cabin'].str.extract(pat=r'([0-9]+)', expand=False),\n",
    "            errors='coerce'\n",
    "            )\n",
    "        return X_copy\n",
    "    \n",
    "feature_engineering_pipeline = Pipeline(steps=[\n",
    "    ('title_extractor', TitleExtractor()),\n",
    "    ('age_imputer', AgeImputer()),\n",
    "    ('cabin_location_extractor', CabinLocationExtractor())\n",
    "])\n",
    "\n",
    "FARE_TO_LOG_TRANS = ['Fare']\n",
    "CAT_FEATURES = ['Sex',\n",
    "                        'Embarked',\n",
    "                        'Pclass',\n",
    "                        'Title_feat',\n",
    "                        'Deck_feat'\n",
    "                        ]\n",
    "AGE_TO_BIN = ['Age']\n",
    "ZONE_TO_BIN = ['Zone_feat']\n",
    "\n",
    "\n",
    "fare_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(np.log1p)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Simple function for pd.cut\n",
    "def apply_pd_cut(X, bins, labels):\n",
    "    series = pd.Series(X[:, 0])\n",
    "    binned_series = pd.cut(series, bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "    return binned_series.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Bins for Age\n",
    "# Infant: 0-5, Child: 6-12, young-adult: 13-25, adult: 26-50, elder: 51->\n",
    "AGE_BINS = [0, 5, 12, 25, 50, np.inf]\n",
    "AGE_LABELS = ['Infant', 'Child', 'Young-Adult', 'Adult', 'Senior']\n",
    "\n",
    "age_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('binner', FunctionTransformer(apply_pd_cut, kw_args={'bins': AGE_BINS, 'labels': AGE_LABELS})),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "def apply_bin_zone(X, n_bins):\n",
    "    series = pd.Series(X[:, 0])\n",
    "    labels  = [f'Q{i}' for i in range(1, n_bins + 1)]\n",
    "    binned_series = pd.qcut(series, q=n_bins, labels=labels, duplicates='drop')\n",
    "\n",
    "    # Convert nans to unknown\n",
    "    binned_series = binned_series.cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "    return binned_series.to_numpy().reshape(-1, 1)\n",
    "\n",
    "zone_pipeline = Pipeline(steps=[\n",
    "    ('bin_with_missing_values', FunctionTransformer(apply_bin_zone, kw_args={'n_bins': 8})),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('fare_log', fare_pipeline, FARE_TO_LOG_TRANS),\n",
    "        ('cat', categorical_pipeline, CAT_FEATURES),\n",
    "        ('age_binned', age_pipeline, AGE_TO_BIN),\n",
    "        ('zone_binned', zone_pipeline, ZONE_TO_BIN)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "grand_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', feature_engineering_pipeline),\n",
    "    ('preprocessing', preprocessor)\n",
    "])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a904144",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Evaluation\n",
    "\n",
    "---\n",
    "With our pre-processing pipeline fully constructed, we can now proceed to the modeling phase. The first step is to establish a performance baseline by evaluating several different classification algorithms. This process will help us identify the most promising model architecture before investing time in hyperparameter tuning.\n",
    "\n",
    "As established in the introduction, our primary metric for model selection will be the F1-Score, as it provides a balanced measure of a model's performance. For a more complete picture, we will also evaluate Precision, Recall, and overall Accuracy.\n",
    "\n",
    "To obtain a reliable estimate of each model's generalization performance, we will use 5-fold cross-validation. This method helps mitigate the risk of overfitting to a single train-test split and provides a more robust foundation for our subsequent decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ed5aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline-evaluation of models...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 734, in fit_transform\n    return last_step.fit(Xt, y, **last_step_params[\"fit\"]).transform(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'transform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     29\u001b[39m full_pipeline = Pipeline(steps=[\n\u001b[32m     30\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, grand_pipeline),\n\u001b[32m     31\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, model)\n\u001b[32m     32\u001b[39m ])\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Evaluate f1-score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m f1_scores = cross_val_score(full_pipeline, X_train, y_train, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = cross_validate(\n\u001b[32m    685\u001b[39m     estimator=estimator,\n\u001b[32m    686\u001b[39m     X=X,\n\u001b[32m    687\u001b[39m     y=y,\n\u001b[32m    688\u001b[39m     groups=groups,\n\u001b[32m    689\u001b[39m     scoring={\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: scorer},\n\u001b[32m    690\u001b[39m     cv=cv,\n\u001b[32m    691\u001b[39m     n_jobs=n_jobs,\n\u001b[32m    692\u001b[39m     verbose=verbose,\n\u001b[32m    693\u001b[39m     params=params,\n\u001b[32m    694\u001b[39m     pre_dispatch=pre_dispatch,\n\u001b[32m    695\u001b[39m     error_score=error_score,\n\u001b[32m    696\u001b[39m )\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:431\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    411\u001b[39m results = parallel(\n\u001b[32m    412\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    413\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    429\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:517\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    511\u001b[39m     all_fits_failed_message = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m     some_fits_failed_message = (\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonaslilletvedt/miniconda3/envs/titanic-survival-prediction/lib/python3.11/site-packages/sklearn/pipeline.py\", line 734, in fit_transform\n    return last_step.fit(Xt, y, **last_step_params[\"fit\"]).transform(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'transform'\n"
     ]
    }
   ],
   "source": [
    "# Import SKlearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set seed for reproducible results\n",
    "seed = 43\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=seed, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=seed),\n",
    "    'Gradient Boostind': GradientBoostingClassifier(random_state=seed), \n",
    "    'Support Vector Machine': SVC(random_state=seed, probability=True)\n",
    "}\n",
    "\n",
    "# Dict for results\n",
    "results = {}\n",
    "\n",
    "print('Starting baseline-evaluation of models...')\n",
    "\n",
    "# Iterate over all models\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    # Train model using the pre-constructed pipeline\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', grand_pipeline),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Evaluate f1-score\n",
    "    f1_scores = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c1d87",
   "metadata": {},
   "source": [
    "For our baseline we will use four different classification models. \n",
    "1. **Logistic Regression**\n",
    "2. **K-Nearest Neighbors**\n",
    "3. **Random Forest**\n",
    "4. **Gradient Boosting**\n",
    "5. **Support Vector Machine**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic-survival-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
